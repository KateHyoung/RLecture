
#####################################
#### 빅 데이터와 머신러닝 R 코드 ####
#####################################

## -- 한글 코딩문제 -- ##
# MAC의 경우: Preference->Code->Saving->Change to UTF-8
# Window경우:file -> Reopen witht encoding... -> UTF-8 선택  -> ok

# R studio와 R 프로그램을 인터넷에서 다운받아 설치하세요.
# 일부 컴퓨터의 경우 한글이 깨질수가 있는데, 이 경우 CP949로 다시 인코딩하세요.
# 이전 세션에서 불러왔던 데이터 및 라이브러리를 제거하고 새로운 세션을 시작하려면 아래의 명령어를 실행해주세요.
rm(list=ls())

# Working Directory 설정하기
setwd("C:\\....")
# 에러발생을 최소화 하기 위해 경로명에 한글이 없도록 합니다.    
# 예: setwd("c:/rData") 
# 다른 데이터 파일을 지정한 폴더 안에 넣어놓으면 편하게 코드를 실행시킬 수 있습니다.


#######################################################
####### A small Project: document classification ######
#######################################################



# 필요한 패키지
install.packages("caret")
install.packages("tm")
install.packages("dplyr")

# workding directory 설정


# 데이터 가져오기
sms_raw <- read.csv("sms_spam.csv", quote = "\"'", stringsAsFactors = FALSE)

# 데이터 확인
str(sms_raw)

# type 항목을 문자형 벡터로 변환
sms_raw$type <- factor(sms_raw$type)

# 다시 데이터 확인
str(sms_raw$type)

# type 변수확인
table(sms_raw$type)

# 텍스트 데이터들은 반들이 전처리 과정을 거처 이후 머신러닝분석이 가능할 수 있게 해야 합니다. 

## 데이터 전처리 
# 텍스트 데이터는 의미 없는 단어, 공백, 숫자, 구두점 등으로 수어되어 있으므로 이런 문자들을 먼저 제거하는 과정을 거처
# 문장을 개별 단어로 나누는 방법을 고려해봅니다.

# tm 패키지 사용
library(tm)

# 텍스트 데이터 처리는 우선 코퍼스(corpus)를 생성하는 것 부터 시작합니다. 
# 코퍼스는 텍스트 문서의 모음입니다.

# 코퍼스 생성

# 생성된 코퍼스는 컴퓨터 디스크에 저장되기 보다는 컴퓨터 메모리에 저장됨에 유의하시기 바랍니다.
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
# VCorpus 함수에 대한 자세한 내용은 다음 문서를 불러 사용하세요. 
vignette("tm")

# 그럼 생성된 코퍼스를 확인해 보겠습니다.
print(sms_corpus)

# 우리가 원하는 텍스트 데이터가 잘 들어왔는지 확인해 보기 위해서는 inspect() 함수를 사용해야 합니다.
inspect(sms_corpus[1:3])
# 아직 문자 즉 텍스트의 내용은 확인이 불가하네요. 

# 내용은 저장한 코퍼스를 문자함수로 만들어 보여줘야 합니다.
as.character(sms_corpus[[1]])

# 몇개의 텍스트를 들을 한꺼번에 보려면 어떻게 해야 할까요?
# lapply() 함수를 사용합니다.
?lapply()

lapply(sms_corpus[1:3], as.character)


# 어떤 문자들이 있나요? 앞서 잠깐 살펴봤지만, 이런 문자들이 우리가 하고자 하는 분석에 다 필요할까요?

# 텍스트 분석의 전처리는 이런 것들을 정리하는 전부를 말합니다. 

# 확인해 보시면 아직 구두점, 반복되는 문자 (예를 들어 the, a, and)등이 아직 있는 것을 확인하실 수 있습니다. 
# 이것을 정리하는 과정을 거치기 위해서는 tm_map()함수를 사용하게 됩니다. 

# 영문텍스트의 대문자, 소문자를 한가지로 통일해 주는 작업을 하도록 하겠습니다.
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
# tolower() 함수는 모든 영문자를 소문자로 바꿔주고,conttnet_trasformer()함수는 코퍼스에 특정함수를 적용하는 참조함수 입니다.

# 그럼 새로 생성된 함수를 확인해 보도록 하겠습니다.
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])

# 이번에는 SMS메시지의 숫자함수를 제거해보도록 하겠습니다. 
#(만약 숫자정보가 문서 분류에 필요하다면 숫자를 제거하면 안되겠죠?)
# 이 부분은 연구자가 신중하게 판단해야 합니다.
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
# 주의: 이번 코드에서는 content_transformer()사용하지 않았습니다. 왜 그럴까요? 
# removeNumers()에 참조함수가 필요없기 때문입니다. 

# 이번에는 의미적으로 필요없는 단어를 제거해 보도록 하겠습니다. (예, to, and, but, or, a, etc.)
# 이런 단어를 불용어(stop words)라고 합니다.
# stopwords()함수 사용

sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords("english"))
# 시간이 걸릴 수도 있습니다. 또 불용어를 우리가 지정할 수 도 있습니다. 

# 이번에는 구두점 (punctuation)을  제거해 보도록하겠습니다.
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)

# 구두점을 제거할 때 유의할 부분이 있습니다. 
removePunctuation("hello....world")
# 결과가 어떤가요? 만약 hello와 world 단어가 각각 의미를 지닌 단어가 되다면, 구두점을 제거하는 것은 분석에 중요한 정보를
# 잃을 수 있습니다. 추후 연구하실 때 주의하세요.

# 영어에는 형태소 (stemming) 가 있습니다 (예, learn: learned, learning)
# 이런 단어의 기본 단어로 형태를 전환시켜주는 R 패키지가 SnowballC입니다. 
# 컴퓨터에 이 패키지가 없으면 설치해 주세요.

# install.packages("SnowballC")
library(SnowballC)

# 이제 기본형태소를 반환하는 함수를 테스트 해 보겠습니다.
wordStem(c("learn", "learned", "learning", "learns"))

# 위의 함수를 이용해서 다시 sms_corpus를 정리해 보도록 하겠습니다.
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
# 만약, 스케줄된 모든 코어에 오류 발생 (all scheduled cores encountered errors)'라는 메세지가 리턴되면, 싱글 코어에 tm_map()
# 명령을 강제할 수 있도록 mc.cores=1을 함수안에 지정해 주세요.

# 문자데이터의 마지막으로 빈칸을 정리하는 과정을 거치도록 하겠습니다.
# stripWhitespace()함수 사용
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)

# 다시한번 원문자와 정리된 문자를 비교해 보도록 하겠습니다. 
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])

# 두 데이터의 차이가 보이시나요? 이제 문자데이터의 전처리 과정이 다 끝났습니다. 


##-- 텍스트 문자를 단어로 쪼게기
# 먼저 정리된 데이터는 분석을 위한 작은 단위로 쪼게야 합니다. 이를 토큰화 (tokenization)이라고 합니다.
# tm패키지의 DocumentTermMatrix()함수 사용
# 이 함수는 문서-단어 행렬(Document Term Matrix, DTM)의 구조를 만들어 줍니다. 
# 이 행렬의 행은 문서, 그리고 열은 단어를 의미합니다. (그림으로 설명)

# 일단, DTM을 생성하도록 하겠습니다.
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
# 위 명령어는 기본 설정을 이용, 토큰화된 코퍼스를 DTM 객체로 만들어줍니다. 

## 지금까지의 전처리 과정과 DTM을 한 명령어로 만들어 줄 수 있기도 합니다.
sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(
  tolower = TRUE,
  removeNumbers = TRUE,
  stopwords = TRUE,
  removePunctuation = TRUE,
  stemming = TRUE
))
# 실행하는 데 시간이 걸립니다.

# 두개의 처리결과를 비교해 볼까요?
sms_dtm
sms_dtm2

# 두개의 처리결과가 약간 다르다는 것을 알 수 있습니다. 
# 이 원인은 전처리 단계의 순서에서 찾을 수 있습니다. 
# 이 부분에서 연구자가 신중하게 각 전처리 단계의 순서를 결정해야 하는 주의를 기울여야 합니다.


##--데이터 셋 나누기: 트레이닝 셋, 테스트 셋
# 주의: 연구자는 먼저 데이터를 나누고 전처리를 할 수 있습니다. 이 때에 두 데이터의 전처리 과정이 같음을 확인해야
# 분석 결과의 신뢰성이 보장될 것입니다. 

# 훈련용 데이터: 70%, 테스트용 데이터 30%
# 5574*.7 = 3902
# 5574*.3 = 1672

sms_dtm_train <- sms_dtm[1:3902, ]
sms_dtm_test <- sms_dtm[3903:5574, ]

# 나중에 모델평가시 편의를 위해 테스트 데이터의 분류결과값을 벡터로 저장해 두도록 하겠습니다.
sms_train_labels <- sms_raw[1:3902, ]$type
sms_test_labels <- sms_raw[3903:5574, ]$type

# 두 데이터 셋의 대표성을 살펴보겠습니다.
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))

# 어떤가요? 균등하게 분포되어 있나요?

##-- 문제데이터를 보여주기
# word cloud는 문자 데이터의 빈도를 눈으로 보기 쉽도록 도와줍니다. 
# 빈도수가 높은 단어는 큰 글자폰트로, 낮은 단어는 작은 폰트로 시각화합니다. 
# wordcloud 패키지 사용

# install.packages("wordcloud")
library(wordcloud)

wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
# random.order는 FLASE로 명시했으므로 빈도가 높은 단어가 중심에 더 가깝게 단어들이 나열 됩니다.
# min.freq는 단어가 구름(cloud)에 보이기 전에 코퍼스에서 나타나는 횟수를 의미하는데, 50번 이상 언급된 단어만
# 그림으로 표현이 된다는 의미입니다. (50개 = 1%)
# 자세한 설명은 
?wordcloud

# 그렇다면 이제부터 spam과 ham 그룹의 단어 구성이 어떻게 다른지 그림으로 살펴보도록 하겠습니다.
spam <- subset(sms_raw, type == "spam")
ham <- subset(sms_raw, type = "ham")

wordcloud(spam$text, max.words = 30, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
# max.word는 가장일반적인 단어 수를 의미, scale는 그려지는 단어의 최대, 최소 크기의미

# 그림만 보고 어떤 그림이 스팸메세지의 단어구조라고 생각하시나요? 


##########
# 다시 DTM으로 돌아가 보겠습니다.
# DTM안에 0값이 많으면, 이 행렬을 희소행렬(sparse matrix)이라고 합니다. 즉, 행렬의 대다수 셀이 0
# 으로 구성된 것을 말합니다. 왜 이런 구조가 발생할까요?
##########

inspect(sms_dtm_train[101:110, 200:210])

# 위 행렬처럼 0이 많은겨우, 즉 희소행렬인 경우 특정모델을 적용했을 경우 훈련(training)이 제대로 이뤄지지 않을 수 있습니다.
# 왜냐하면 0에 관한 정보가 너무 많아 진짜로 필요한 정보들이 희석될 가능성이 있기 때문입니다.
# 그래서 전체  sms중 어느정도 빈도수가 있는 단어를 가지고 나이브베이즈 모형을 훈련시켜야 합니다.

# 예를 들어 지금 이 연구의 경우에는 데이터에서 약 0.1% 보다 적게 나타다는 단어들은 제거하려고 합니다.

# 트레이닝 데이터 셋에서 최소 다섯번 나타나는 단어 저장하기
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
# sms_freq_words 벡터를 살펴보세요.

# 이제 트레이닝 데이터 셋에서 자주 나타나는 단어가 있는 열, 즉 sms만 선택할 것입니다.
sms_dtm_freq_train <- sms_dtm_train[ , sms_freq_words]
inspect(sms_dtm_freq_train)
# 테스트 데이터 셋도 같은 방법으로 새로운 데이터 셋을 선택하도록 하겠습니다.
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
inspect(sms_dtm_freq_test)
# sms_freq_words 벡터의 단어수와 새 행렬의 열의 수를 비교해보세요. 어떤가요?

# 나이브 베이즈 문서분류는 주로 범주형 데이터에 대해 훈련이 됩니다. 
# 특정 단어가 그 문서에 나타나는지에 대해 '예', '아니오'로 표시되는 범주형 자료가 씌여지기 
# 때문에 다음과 같은 함수가 필요합니다.
convert_counts <- function(x) {
  x <- ifelse(x>0, "Yes", "No")
}

# 위의 함수를 dtm의 각 열에 적용하기 위해서 apply() 함수가 사용됩니다.
# 함수에 대한 자세한 사항은 다음과 같이 살펴보세요.
?apply()
# 이때 MARGIN 파라미터를 사용하게 되는, 값이 1이면 행에 대애서 특정 함수를 적용하고, 값이 2이면 열에 대해서 특정함수를
# 적용합니다.

sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
sms_train[3:8, 3:8]

## -- 나이브베이즈 모델 적용
# 이제 데이터가 준비가 됐으므로, 나이브베이즈 모델을 데이터에 적용시켜 보도록 하겠습니다.
# 단어의 빈도수와 구성을 통해서 각 sms가 spam인지 ham인지를 구분하게 됩니다.
# 이 실행은 e1071패키지에 의해서 구현됩니다.

install.packages("e1071")
library(e1071)

sms_classifier <- naiveBayes(sms_train, sms_train_labels)

# 이제 우리는 나이브베이즈 알고리즘을 훈련시킨 문서분류기(classfier)를 만들었습니다. 
# 이 분류기 성능을 시험test해 보겠습니다.
# 데이터는 test데이터를 이용합니다.

# 테스트를 할 땐, predict() 함수가 사용됩니다.
sms_test_pred <- predict(sms_classifier, sms_test)
# 다소 시간이 걸릴 수도 있습니다. 

# 이제 우리가 만든 문서분류기 성능이 얼마나 좋은지 살펴보겠습니다.

# 테스트 셋에 저장된 문서분류값과 문서분류기 결과값을 비교해 보기로 하겠습니다.

# install.packages("gmodels")
library(gmodels)
CrossTable(sms_test_pred, sms_test_labels, prop.chisq = F, prop.t = F, prop.r = F,
           dnn = c("predicted", "actural"))

# 잘 못 분류된 문서의 갯수: 9 + 25 = 34, 따라서 오류는 34/1672 = .0203349, 약 2%가 되겠습니다.

#### 좀 더 모델 성능을 개선시켜 보겠습니다.
# 우리는 나이브베이즈 알고리듬을 적용시킬 때, 어떤 파라미터도 설정하지 않았습니다. 그럼 파라미터 조절을
# 통해 모델성능이 더 좋아질 수있도록 그 값을 수정해 보도록 하겠습니다.
?naiveBayes

sms_classifier2 <- naiveBayes(sms_train, sms_train_labels, laplace = 0.5)

# 예측을 해보겠습니다.
system.time(sms_test_pred2 <- predict(sms_classifier2, sms_test))
# 다소 시간이 걸릴 수 있습니다.

# 이제 성능이 얼마나 좋은지 분류결과를 살펴보도록 하겠습니다.
CrossTable(sms_test_pred2, sms_test_labels, prop.chisq = F, prop.c = F, prop.t = F, 
           dnn = c("predicted", "acual"))

# 잘못 분류된 문서 = 7 + 25 = 32, 분석기의 오류가 오히려 증가했습니다. 오류율 약 1.9% 

